{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Demo - Support Vector Machines on PYNQ\n",
    "**Ryan Greer, \n",
    "19/03/2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "- 3 hyperspectral images of Streptomyces strains for training and testing\n",
    "- Training data (**KNOWN CLASSES**) and testing data (**UNKNOWN CLASSES**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DATA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and plot in 3D space\n",
    "- The data originally had 256 variables associated with each pixel\n",
    "- Using principle components analysis (PCA), data has been compressed to 3 dimensions\n",
    "- First 3 dimensions of PCA data used as SVM argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_matrix():\n",
    "    f = open(\"training_matrix.dat\",\"r\")\n",
    "\n",
    "    contents = f.read()\n",
    "    training_mat_data = contents.split()\n",
    "    #x = np.array(training_mat_data)\n",
    "    #training_out = np.asfarray(x,np.float32)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return training_mat_data\n",
    "    \n",
    "def get_testing_matrix():\n",
    "    f = open(\"test_matrix.dat\",\"r\")\n",
    "\n",
    "    contents = f.read()\n",
    "    testing_mat_data = contents.split()\n",
    "    #x = np.array(testing_mat_data)\n",
    "    #self.training_mat_data_fi_uint16 = np.asarray(x,np.uint16)\n",
    "    \n",
    "    return testing_mat_data\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_mat_data = get_training_matrix()\n",
    "training_plot_data = np.transpose(np.reshape(training_mat_data,(150,3)))\n",
    "training_plot_data_new = training_plot_data.tolist()\n",
    "\n",
    "#training_plot_data_x[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax = plt.axes(projection='3d')\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "training_plot_data_x_1 = [float(i) for i in training_plot_data_new[0][0:50]]\n",
    "training_plot_data_x_2 = [float(i) for i in training_plot_data_new[0][50:100]]\n",
    "training_plot_data_x_3 = [float(i) for i in training_plot_data_new[0][100:150]]\n",
    "\n",
    "training_plot_data_y_1 = [float(i) for i in training_plot_data_new[1][0:50]]\n",
    "training_plot_data_y_2 = [float(i) for i in training_plot_data_new[1][50:100]]\n",
    "training_plot_data_y_3 = [float(i) for i in training_plot_data_new[1][100:150]]\n",
    "\n",
    "training_plot_data_z_1 = [float(i) for i in training_plot_data_new[2][0:50]]\n",
    "training_plot_data_z_2 = [float(i) for i in training_plot_data_new[2][50:100]]\n",
    "training_plot_data_z_3 = [float(i) for i in training_plot_data_new[2][100:150]]\n",
    "\n",
    "ax.scatter(training_plot_data_x_1, training_plot_data_y_1, training_plot_data_z_1, c='red', s=1, alpha=1)\n",
    "ax.scatter(training_plot_data_x_2, training_plot_data_y_2, training_plot_data_z_2, c='green', s=1, alpha=1)\n",
    "ax.scatter(training_plot_data_x_3, training_plot_data_y_3, training_plot_data_z_3, c='blue', s=1, alpha=1)\n",
    "\n",
    "\n",
    "ax.set_xlabel('1st principle component')\n",
    "ax.set_ylabel('2nd principle component')\n",
    "ax.set_zlabel('3rd principle component')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training bitstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETUP load the overlay\n",
    "from pynq import Overlay\n",
    "\n",
    "overlay = Overlay(\"/home/xilinx/jupyter_notebooks/PROJECT_FULL_DEMO/SMO_FULL_PYNQ_Z2.bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the training drivers\n",
    "- two drivers:\n",
    "- 1) parse files containing dataset\n",
    "- 2) format data to correct format and stream through PL/obtain results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq import DefaultIP\n",
    "import numpy as np\n",
    "\n",
    "class parse_files():\n",
    "    def __init__(self):\n",
    "        #super().__init__()\n",
    "        # dot product matrix will be computed from two training matrix streams \n",
    "        self.training_labels_data_fi_uint8 = None\n",
    "        self.training_mat_data_fi_uint16 = None\n",
    "        self.input_details_data_fi_uint32 = None\n",
    "        \n",
    "        # miscellaneous variables\n",
    "        self.no_training_vectors_fi_uint32 = None\n",
    "        self.no_training_vectors_int = None\n",
    "        self.no_variables_fi_uint32 = None\n",
    "        self.no_variables_int = None\n",
    "        self.C_fi_uint32 = None\n",
    "        self.tolerance_fi_uint32 = None\n",
    "        # number of classifiers:\n",
    "        self.no_classes = None\n",
    "        \n",
    "    def get_training_labels(self):\n",
    "        # for self checking Python tests\n",
    "        f = open(\"training_labels.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        training_labels_data = contents.split()\n",
    "        x = np.array(training_labels_data)\n",
    "        self.training_labels_data_fi_uint8 = np.asarray(x,np.uint8)\n",
    "\n",
    "        f.close()    \n",
    "        \n",
    "    def get_training_matrix(self):\n",
    "        f = open(\"training_matrix_fi.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        training_mat_data = contents.split()\n",
    "        x = np.array(training_mat_data)\n",
    "        self.training_mat_data_fi_uint16 = np.asarray(x,np.uint16)\n",
    "\n",
    "        f.close()\n",
    "        \n",
    "    def get_input_details(self):\n",
    "        f = open(\"training_details.dat\",\"r\")\n",
    "        \n",
    "        contents = f.read()\n",
    "        input_details_data = contents.split()\n",
    "        x = np.array(input_details_data)\n",
    "        self.input_details_data_float = np.asfarray(x,np.float32)\n",
    "        \n",
    "        self.no_training_vectors_float = self.input_details_data_float[0]\n",
    "        self.no_training_vectors_int = int(self.no_training_vectors_float)\n",
    "        self.no_variables_float = self.input_details_data_float[1]\n",
    "        self.no_variables_int = int(self.no_variables_float)\n",
    "        self.C = self.input_details_data_float[2]\n",
    "        self.tolerance = self.input_details_data_float[3]\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "        f = open(\"training_details_fi.dat\",\"r\")\n",
    "        \n",
    "        contents = f.read()\n",
    "        input_details_data = contents.split()\n",
    "        x = np.array(input_details_data)\n",
    "        self.input_details_data_fi_uint32 = np.asarray(x,np.uint32)\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "    def get_no_classes(self):\n",
    "        f = open(\"no_classes.dat\")\n",
    "        \n",
    "        contents = f.read()\n",
    "        self.no_classes = contents.split()\n",
    "        self.no_classes = int(self.no_classes[0])\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "import pynq.lib.dma\n",
    "import struct\n",
    "\n",
    "from pynq import allocate\n",
    "\n",
    "class SMO_driver(parse_files):\n",
    "    def __init__(self):\n",
    "        #super().__init__()\n",
    "        \n",
    "        self.get_no_classes()\n",
    "        self.no_classifiers = int(0.5 * self.no_classes * (self.no_classes - 1))\n",
    "        \n",
    "        # DECLARE MEMORY FOR STACK\n",
    "        self.training_labels_buffer = None\n",
    "        self.training_matrix_buffer = None\n",
    "        self.input_details_buffer = None\n",
    "        self.alpha_out_buffer = None\n",
    "        self.output_details_buffer = None\n",
    "\n",
    "        # DMA library instantiate\n",
    "        #self.dma_dp_o = overlay.SMO_1.dma_dp_o\n",
    "        #self.dma_dp_i = overlay.SMO_1.dma_dp_i\n",
    "        #self.dma_tl = overlay.SMO_1.dma_tl\n",
    "        #self.dma_tm_o = overlay.SMO_1.dma_tm_o\n",
    "        #self.dma_tm_i = overlay.SMO_1.dma_tm_i\n",
    "        #self.dma_id = overlay.SMO_1.dma_id\n",
    "\n",
    "        # these lists contain the DMA instances for the different cores used in the overlay\n",
    "        self.dma_dp_o_1 = []\n",
    "        self.dma_dp_i_1 = []\n",
    "        self.dma_dp_o_2 = []\n",
    "        self.dma_dp_i_2 = []\n",
    "        self.dma_dp_id_o = []\n",
    "        self.dma_dp_id_i = []\n",
    "        self.dma_tl = []\n",
    "        self.dma_tm_o = []\n",
    "        self.dma_tm_i = []\n",
    "        self.dma_id = []\n",
    "        \n",
    "        #self.dma_ao = overlay.SMO_1.dma_ao\n",
    "        #self.dma_kkt = overlay.SMO_1.dma_kkt\n",
    "        #self.dma_od = overlay.SMO_1.dma_od\n",
    "\n",
    "        self.dma_ao = []\n",
    "        self.dma_kkt = []\n",
    "        self.dma_od = []\n",
    "        \n",
    "        self.no_cores = 2\n",
    "        self.classifier_indices = []\n",
    "        self.no_training_vectors_all = np.zeros(shape=(self.no_cores), dtype=np.uint32)\n",
    "        \n",
    "        # create a dispatcher to allow streamlined access to the different cores in the design\n",
    "        SMO_1 = overlay.SMO_1\n",
    "        SMO_2 = overlay.SMO_2\n",
    "        \n",
    "        self.SMO_dispatcher = {\n",
    "            1: SMO_1,\n",
    "            2: SMO_2,\n",
    "        }\n",
    "        \n",
    "        # store training models\n",
    "        self.sv_coeffs = []\n",
    "        self.sv_indices = []\n",
    "        self.no_svs = []\n",
    "        self.offsets = []\n",
    "        self.no_itrs = []    \n",
    "        \n",
    "    def fixed_point_to_float(self, input_, word_length, integer_length):\n",
    "        # returns floating point representation of fixed point SIGNED integer input\n",
    "        # specify the word length and integer length\n",
    "        \n",
    "        fractional_length = word_length - integer_length\n",
    "        output = 0\n",
    "\n",
    "        input_bin_string = \"{0:b}\".format(input_)\n",
    "\n",
    "        for n1 in range(word_length - len(input_bin_string)):\n",
    "            input_bin_string = '0' + input_bin_string\n",
    "    \n",
    "        no_positive = 1\n",
    "\n",
    "        # number is negative\n",
    "        if(input_bin_string[0] == '1'):\n",
    "            no_positive = 0\n",
    "            input_bin_tc = input_ - (1 << word_length)\n",
    "            # input is now negative\n",
    "            input_ = -input_bin_tc\n",
    "    \n",
    "        input_bin_string = \"{0:b}\".format(input_)\n",
    "\n",
    "        for n1 in range(word_length - len(input_bin_string)):\n",
    "            input_bin_string = '0' + input_bin_string\n",
    "    \n",
    "        for i, c in enumerate(input_bin_string):\n",
    "            if(c == '1'):\n",
    "                output = output + 2 ** (integer_length - 1 - i)\n",
    "\n",
    "        if(no_positive == 1):\n",
    "            return output\n",
    "        else:\n",
    "            return -output\n",
    "\n",
    "        # https://stackoverflow.com/questions/699866/python-int-to-binary-string\n",
    "        # https://stackoverflow.com/questions/538346/iterating-each-character-in-a-string-using-python\n",
    "        # https://stackoverflow.com/questions/1604464/twos-complement-in-python\n",
    "        \n",
    "    def int_bits_IEEE754_to_float(self, to_convert):\n",
    "        # credit - https://stackoverflow.com/questions/30124608/convert-unsigned-integer-to-float-in-python\n",
    "        # convert integer bits (unsigned long 'L') (IEEE754 single-precision) to float 'f'\n",
    "        s = struct.pack('>L', to_convert)\n",
    "        return struct.unpack('>f', s)[0]\n",
    "        \n",
    "    def write_training_model_to_files(self, current_classifier):  \n",
    "        # SV COEFFS TO FILE #\n",
    "        # create new numpy array to copy to file - copt pynq buffer into\n",
    "        coeffs_write = np.zeros(shape=(len(self.sv_coeffs[current_classifier-1]),1), dtype=np.uint32)\n",
    "        np.copyto(coeffs_write, self.sv_coeffs[current_classifier-1])\n",
    "        np.savetxt(\"coeffs_fi_\"+str(current_classifier)+\".dat\",coeffs_write,'%d')\n",
    "        \n",
    "        # SUPPORT VECTORS TO FILE #\n",
    "        #svs_write = self.training_mat_data_fi_uint16\n",
    "        #svs_write = np.reshape(svs_write,(len(self.training_mat_data_fi_uint16/self.no_variables_int),self.no_variables_int))\n",
    "        svs_write = np.reshape(self.training_mat_data_fi_uint16,((int(len(self.training_mat_data_fi_uint16)/self.no_variables_int),self.no_variables_int)))\n",
    "        svs_write = svs_write[self.sv_indices[current_classifier-1]]\n",
    "        np.savetxt(\"svs_fi_\"+str(current_classifier)+\".dat\",svs_write,'%d')\n",
    "        \n",
    "        # OFFSET TO FILE #\n",
    "        np.savetxt(\"offset_fi_\"+str(current_classifier)+\".dat\",self.offsets[current_classifier-1],'%d')\n",
    "        \n",
    "    def write_n_svs_to_file(self):\n",
    "        # populate number of support vectors to array and write to file\n",
    "        no_classifiers = int(0.5 * self.no_classes * (self.no_classes - 1))\n",
    "        n_svs = np.zeros(shape=(no_classifiers), dtype=np.uint32)\n",
    "        \n",
    "        for n1 in range(no_classifiers):\n",
    "            n_svs[n1] = len(self.sv_coeffs[n1])\n",
    "            \n",
    "        np.savetxt(\"n_svs.dat\",n_svs,'%d')\n",
    "            \n",
    "    def pynq_buffer_init(self):\n",
    "        # DECLARE MEMORY FOR HEAP - need to use lists as there are multiple buffers needing to be transferred simultaneously \n",
    "        # containing different training sets\n",
    "        \n",
    "        self.training_matrix_buffers = []\n",
    "        self.alpha_out_buffers = []\n",
    "        self.output_details_buffers = []\n",
    "        self.kkt_violation_buffers = []\n",
    "        \n",
    "        self.input_details_dpm_buffers = []\n",
    "        \n",
    "        self.input_details_buffer = allocate(shape=(5,), dtype=np.int32)\n",
    "        \n",
    "        for n1 in range(self.no_cores):\n",
    "            self.output_details_buffer = allocate(shape=(1,), dtype=np.uint32)\n",
    "            # declare buffers to receive indication signals to send new copies of training and dot product matrices\n",
    "            # kkt_violation_buffer checks if there is a kkt violation and we need to execute the p loop\n",
    "            self.kkt_violation_buffer = allocate(shape=(1,), dtype=np.uint8)\n",
    "            self.input_details_dpm_buffer = allocate(shape=(2,), dtype=np.uint16)\n",
    "            \n",
    "            self.output_details_buffers.append(self.output_details_buffer)\n",
    "            self.kkt_violation_buffers.append(self.kkt_violation_buffer)\n",
    "            self.input_details_dpm_buffers.append(self.input_details_dpm_buffer)\n",
    "        \n",
    "    def pynq_buffer_delete(self):\n",
    "        # close buffers - clean up heap memory\n",
    "        \n",
    "        for n1 in range(self.no_cores):\n",
    "            if(n1 == len(self.classifier_indices)):\n",
    "                break\n",
    "            self.training_matrix_buffers[n1].close()\n",
    "            self.input_details_buffer.close()\n",
    "            \n",
    "            self.alpha_out_buffers[n1].close()\n",
    "            self.output_details_buffers[n1].close()\n",
    "        \n",
    "    def SMO_parallel(self, training_matrices, training_labels, index_1, index_2, no_training_vectors, no_variables, C, tolerance, max_itr):\n",
    "        # index_1 is the index of the first training vector with respoect to the entire training dataset\n",
    "        # index_2 is same but for negative class\n",
    "        # lists for the different parallel classifier executions\n",
    "        \n",
    "        # initialise buffers\n",
    "        self.pynq_buffer_init()\n",
    "        no_variables = int(no_variables)\n",
    "        \n",
    "        # store start index of negative class\n",
    "        negative_class_index = np.zeros(shape=(self.no_cores), dtype=np.uint32)\n",
    "        for n1 in range(self.no_cores):\n",
    "            if(n1 == len(self.classifier_indices)):\n",
    "                break\n",
    "            negative_class_index[n1] = np.where(training_labels[n1] == -1)[0][0]\n",
    "            \n",
    "        # instantiate all DMAs\n",
    "        for n1 in range(self.no_cores):\n",
    "            # check if we are out of range of no_classifiers\n",
    "            if(n1 == len(self.classifier_indices)):\n",
    "                break\n",
    "            self.dma_dp_o_1.append(self.SMO_dispatcher[n1+1].dma_dp_o_1)\n",
    "            self.dma_dp_i_1.append(self.SMO_dispatcher[n1+1].dma_dp_i_1)\n",
    "            self.dma_dp_o_2.append(self.SMO_dispatcher[n1+1].dma_dp_o_2)\n",
    "            self.dma_dp_i_2.append(self.SMO_dispatcher[n1+1].dma_dp_i_2)\n",
    "            self.dma_dp_id_o.append(self.SMO_dispatcher[n1+1].dma_dp_id_o)\n",
    "            self.dma_dp_id_i.append(self.SMO_dispatcher[n1+1].dma_dp_id_i)\n",
    "            self.dma_tl.append(self.SMO_dispatcher[n1+1].dma_tl)\n",
    "            self.dma_tm_o.append(self.SMO_dispatcher[n1+1].dma_tm_o)\n",
    "            self.dma_tm_i.append(self.SMO_dispatcher[n1+1].dma_tm_i)\n",
    "            self.dma_id.append(self.SMO_dispatcher[n1+1].dma_id)\n",
    "            self.dma_ao.append(self.SMO_dispatcher[n1+1].dma_ao)\n",
    "            self.dma_kkt.append(self.SMO_dispatcher[n1+1].dma_kkt)\n",
    "            self.dma_od.append(self.SMO_dispatcher[n1+1].dma_od)\n",
    "            \n",
    "        # allocate buffers for each classifier\n",
    "        for n1 in range(self.no_cores):\n",
    "            if(n1 == len(self.classifier_indices)):\n",
    "                break\n",
    "            no_training_vectors[n1] = int(no_training_vectors[n1])\n",
    "            #self.get_input_details(self.classifier_indices[n1])\n",
    "            self.training_matrix_buffer = allocate(shape=(no_training_vectors[n1]*no_variables,), dtype=np.uint16)\n",
    "            self.alpha_out_buffer = allocate(shape=(no_training_vectors[n1],1), dtype=np.uint32)\n",
    "            \n",
    "            self.training_matrix_buffers.append(self.training_matrix_buffer)\n",
    "            self.alpha_out_buffers.append(self.alpha_out_buffer)\n",
    "                \n",
    "        # loop over number of cores to send data which is constant for the current classifier\n",
    "        for n1 in range(self.no_cores):\n",
    "            if(n1 == len(self.classifier_indices)):\n",
    "                break\n",
    "            training_labels_buffer = allocate(shape=(no_training_vectors[n1],), dtype=np.int8)\n",
    "            np.copyto(training_labels_buffer, training_labels[n1])\n",
    "            \n",
    "            # also want to obtain and store the training matrices and dot product mastrices for all classifiers in this parallel iteration\n",
    "            np.copyto(self.training_matrix_buffers[n1], training_matrices[n1])\n",
    "\n",
    "            self.no_training_vectors_all[n1] = no_training_vectors[n1]\n",
    "            \n",
    "            #### INPUT DETAILS\n",
    "            # convert floats to IEEE754 bits format\n",
    "            no_training_vectors_IEEE754 = np.asarray(no_training_vectors[n1], dtype=np.float32).view(np.int32).item()\n",
    "            no_variables_IEEE754 = np.asarray(no_variables, dtype=np.float32).view(np.int32).item()\n",
    "            max_itr_IEEE754 = np.asarray(max_itr, dtype=np.float32).view(np.int32).item()\n",
    "            tol_IEEE754 = np.asarray(tolerance, dtype=np.float32).view(np.int32).item()\n",
    "            C_IEEE754 = np.asarray(C, dtype=np.float32).view(np.int32).item()\n",
    "                    \n",
    "            # INPUT_DETAILS\n",
    "            # no_training_vectors - from file\n",
    "            # no_variables - from file\n",
    "            # max_itr, tolerance, C - specified by user\n",
    "            x = [no_training_vectors_IEEE754, no_variables_IEEE754, max_itr_IEEE754, tol_IEEE754, C_IEEE754]\n",
    "            np.copyto(self.input_details_buffer, np.asarray(x, np.int32))\n",
    "            \n",
    "            # INPUT_DETAILS for matrix multiply cores - these are integer, not float\n",
    "            x = [no_training_vectors[n1], no_variables]\n",
    "            np.copyto(self.input_details_dpm_buffers[n1], np.asarray(x, np.uint16))\n",
    "            \n",
    "            # transfer input details and training labels to DMA\n",
    "            # send channels:\n",
    "            self.dma_id[n1].sendchannel.transfer(self.input_details_buffer)\n",
    "            self.dma_tl[n1].sendchannel.transfer(training_labels_buffer)\n",
    "            self.dma_id[n1].sendchannel.wait()\n",
    "            self.dma_tl[n1].sendchannel.wait()\n",
    "            \n",
    "            # transfer nput details for matrix multiply core\n",
    "            #self.dma_dp_id[n1].sendchannel.transfer(self.input_details_dpm_buffers[n1])\n",
    "            #self.dma_dp_id[n1].sendchannel.wait()\n",
    "            \n",
    "            # receive channels\n",
    "            self.dma_ao[n1].recvchannel.transfer(self.alpha_out_buffers[n1])\n",
    "                        \n",
    "        # if this is 0, the design has exited without changed_alphas = 0 meaning its iterations have saturated\n",
    "        # if it is 1, then we requiured less iterations than specified\n",
    "        # this parameter is used to determing the last element in the \"output_details\" stream - the last element should be the offset\n",
    "        changed_alphas_exit = np.zeros(shape=(self.no_cores,), dtype=np.uint32)\n",
    "                            \n",
    "        # iterate over the maximum number of iterations\n",
    "        for n0 in range(max_itr):\n",
    "            # iterate over the cores\n",
    "            for n0_1 in range(self.no_cores):\n",
    "                if(n0_1 == len(self.classifier_indices)):\n",
    "                    break\n",
    "                if(changed_alphas_exit[n0_1] == 0):\n",
    "                    self.dma_od[n0_1].recvchannel.transfer(self.output_details_buffers[n0_1])\n",
    "                    ##print(\"test1 -> \", n0_1)\n",
    "\n",
    "                    self.dma_dp_id_o[n0_1].sendchannel.transfer(self.input_details_dpm_buffers[n0_1])\n",
    "                    self.dma_dp_id_o[n0_1].sendchannel.wait()\n",
    "            \n",
    "                    # TRANSFER OUTER DOT PRODUCT MATRIX (FIRST TRAINING MATRIX - NEEDED ONCE PER ITERATION):\n",
    "                    self.dma_dp_o_1[n0_1].sendchannel.transfer(self.training_matrix_buffers[n0_1])\n",
    "                    # send initial copy of outer training matrix and dot product matrix\n",
    "                    self.dma_tm_o[n0_1].sendchannel.transfer(self.training_matrix_buffers[n0_1])\n",
    "        \n",
    "            # p loops:\n",
    "            for n1 in range(max(self.no_training_vectors_all)):\n",
    "                # iterate over the cores\n",
    "                for n1_1 in range(self.no_cores):\n",
    "                    if(n1_1 == len(self.classifier_indices)):\n",
    "                        break\n",
    "                    if(changed_alphas_exit[n1_1] == 0):\n",
    "                        if(n1 < self.no_training_vectors_all[n1_1]):\n",
    "                            \n",
    "                            # loop until we see a kkt violation or can execute next iteration of SMO\n",
    "                            self.dma_kkt[n1_1].recvchannel.transfer(self.kkt_violation_buffers[n1_1])\n",
    "                            \n",
    "                            # TRANSFER SECOND TRAINING MATRIX (TO COMPUTE OUTER DOT PRODUCT MATRIX) - ONCE PER P LOOP\n",
    "                            self.dma_dp_o_2[n1_1].sendchannel.transfer(self.training_matrix_buffers[n1_1])\n",
    "                            self.dma_dp_o_2[n1_1].sendchannel.wait()\n",
    "\n",
    "                            #print(\"1_test - core: \", n1_1)\n",
    "\n",
    "                            while(1):\n",
    "                                s2mm_status_kkt = self.dma_kkt[n1_1].read(0x34)\n",
    "                                ##print(s2mm_status_kkt)\n",
    "                                if(s2mm_status_kkt == 4098):\n",
    "                                    break\n",
    "                    \n",
    "                            ##print(\"KKT violation buffer value: \", self.kkt_violation_buffers[n1_1])\n",
    "                            if(self.kkt_violation_buffers[n1_1] == 1):\n",
    "                                #print(\"2_test\")\n",
    "                                \n",
    "                                # transfer nput details for matrix multiply core\n",
    "                                self.dma_dp_id_i[n1_1].sendchannel.transfer(self.input_details_dpm_buffers[n1_1])\n",
    "                                self.dma_dp_id_i[n1_1].sendchannel.wait()\n",
    "                                # kkt violation - transfer inner matrices\n",
    "                                self.dma_tm_i[n1_1].sendchannel.transfer(self.training_matrix_buffers[n1_1])\n",
    "\n",
    "                                # COMPUTE AND TRANSFER DOT PRODUCT MATRIX:\n",
    "                                self.dma_dp_i_1[n1_1].sendchannel.transfer(self.training_matrix_buffers[n1_1])\n",
    "                                for n1_2 in range(self.no_training_vectors_all[n1_1]):\n",
    "                                    #print(\"3_test\")\n",
    "                                    self.dma_dp_i_2[n1_1].sendchannel.transfer(self.training_matrix_buffers[n1_1])\n",
    "                                    self.dma_dp_i_2[n1_1].sendchannel.wait()\n",
    "                                    \n",
    "                                self.dma_tm_i[n1_1].sendchannel.wait()\n",
    "                                self.dma_dp_i_1[n1_1].sendchannel.wait()\n",
    "                                \n",
    "                            ##else:\n",
    "                                ##print(\"test_2\")\n",
    "                                # no kkt violation\n",
    "                                \n",
    "                        \n",
    "                            ##print(\"test3\")\n",
    "            \n",
    "            # check to see if we should go to next iteration or if alpha has been calculated - we can break\n",
    "            # if we should go to next iteration\n",
    "            # loop over no_cores:\n",
    "            for n0_1 in range(self.no_cores):\n",
    "                if(n0_1 == len(self.classifier_indices)):\n",
    "                    break\n",
    "                if(changed_alphas_exit[n0_1] == 0):\n",
    "                    while(1):\n",
    "                        s2mm_status_od = self.dma_od[n0_1].read(0x34)\n",
    "                        if(s2mm_status_od == 4098):\n",
    "                            test = int(self.output_details_buffers[n0_1])\n",
    "                            ##print(\"iteration \", self.fixed_point_to_float(test,32,12), \" -> \", n0_1)\n",
    "                            break\n",
    "                    \n",
    "                    if(int(self.fixed_point_to_float(test,32,12)) == (n0 + 1)):\n",
    "                        # exiting with \"changed_alphas =/= 0\" (on last iteration)\n",
    "                        ##print(\"test6\")\n",
    "                        changed_alphas_exit[n0_1] = 0\n",
    "                        #continue\n",
    "                    else:\n",
    "                        # exiting with \"changed_alphas == 0\"\n",
    "                        ##print(\"test7\")\n",
    "                        changed_alphas_exit[n0_1] = 1\n",
    "                        #break\n",
    "            \n",
    "            # check if all training models are completed - if any remain, continue\n",
    "            # if all complete, break_all goes to 1 - can read results from all classifiers\n",
    "            break_all = 0\n",
    "            for n0_1 in range(self.no_cores):\n",
    "                if(n0_1 == len(self.classifier_indices)):\n",
    "                    break\n",
    "                if(changed_alphas_exit[n0_1] == 0):\n",
    "                    break\n",
    "                if(n0_1 == (self.no_cores - 1)):\n",
    "                    break_all = 1\n",
    "            \n",
    "            if(break_all == 1):\n",
    "                ##print(\"TEST_8\")\n",
    "                break\n",
    "            \n",
    "        # loop over cores - get results\n",
    "        for n0 in range(self.no_cores):\n",
    "            if(n0 == len(self.classifier_indices)):\n",
    "                break\n",
    "            if(changed_alphas_exit[n0] == 0):\n",
    "                self.dma_od[n0].recvchannel.transfer(self.output_details_buffers[n0])\n",
    "                self.dma_od[n0].recvchannel.wait()\n",
    "                ##print(self.output_details_buffers[n0])\n",
    "            ##else:\n",
    "                ##print(self.output_details_buffers[n0])\n",
    "        \n",
    "            # DMA wait\n",
    "            self.dma_ao[n0].recvchannel.wait()\n",
    "            \n",
    "            # NEED TO OBTAIN COEFFICIENTS - ALPHAS OF NEGATIVE CLASS SHOULD BE MULTIPLIED BY -1\n",
    "            coeffs_temp = self.alpha_out_buffers[n0]\n",
    "            coeffs_temp[negative_class_index[n0]:no_training_vectors[n0],0] = coeffs_temp[negative_class_index[n0]:no_training_vectors[n0],0] * -1\n",
    "            # set very small coefficients to zero\n",
    "            coeffs_temp[np.where(np.absolute(coeffs_temp) < 0.00001)] = 0\n",
    "            coeffs_temp_2 = np.zeros(shape=(len(np.where(coeffs_temp != 0)[0])), dtype=np.uint32)\n",
    "            coeffs_temp_2 = coeffs_temp[np.where(coeffs_temp != 0)[0]]\n",
    "            #print(coeffs_temp_2)\n",
    "            self.sv_coeffs.append(coeffs_temp_2)\n",
    "            self.offsets.append(self.output_details_buffers[n0])\n",
    "            \n",
    "            # GET INDICES OF SUPPORT VECTORS WITH RESPECT TO ENTIRE TRAINING SET\n",
    "            sv_indices_old = np.where(coeffs_temp != 0)[0]\n",
    "            \n",
    "            length_class_1 = negative_class_index[n0]\n",
    "            length_class_2 = no_training_vectors[n0] - length_class_1\n",
    "            \n",
    "            #print(sv_indices_old)\n",
    "            first_classifier_indices = np.where(sv_indices_old < length_class_1)[0]\n",
    "            second_classifier_indices = np.where(sv_indices_old >= length_class_1)[0]\n",
    "            \n",
    "            sv_indices_new = np.zeros(shape=(len(sv_indices_old)), dtype=np.uint8)\n",
    "            \n",
    "            # these lines get the actual indices corresponding to the support vectors identified from the binary training\n",
    "            sv_indices_new[first_classifier_indices] = sv_indices_old[first_classifier_indices] + index_1[n0]\n",
    "            sv_indices_new[second_classifier_indices] = sv_indices_old[second_classifier_indices] - length_class_1 + index_2[n0]\n",
    "            \n",
    "            self.sv_indices.append(sv_indices_new)\n",
    "            #self.alphas.append(self.alpha_out_buffers[n0])\n",
    "            #self.offsets.append(self.output_details_buffers[n0])\n",
    "            \n",
    "            #print(self.alpha_out_buffers[n0])\n",
    "        \n",
    "        self.pynq_buffer_delete()\n",
    "        \n",
    "    def SMO_driver_top(self, C, tolerance, max_itr):\n",
    "        # this function calls the \"SMO_parallel\" driver function to execute (no_cores) runs of the SMO in parallel\n",
    "        # it populates the \"classifier_instances\" variable with the relevant numbers - e.g. if we had 5 classifiers,\n",
    "        # and 2 cores, \"classifier_instances\" would take the values [1,2] on the first iteration, [3,4] on the second\n",
    "        # and [5] on the third\n",
    "        \n",
    "        # this function also gets the indices for each classifier\n",
    "        # e.g. the labels need to be changed to +1 and -1 and the correct classes of the full training matrix need to be used\n",
    "        \n",
    "        # THIS LIST CONTAINS TRAINING MATRICES AND TRAINING LABELS\n",
    "        training_matrices = []\n",
    "        training_labels = []\n",
    "        no_training_vectors_all = []\n",
    "        training_data_1_ind_all = []\n",
    "        training_data_2_ind_all = []\n",
    "        \n",
    "        # keep track of which classifiers we are working on\n",
    "        self.classifier_indices = []\n",
    "        \n",
    "        self.get_training_matrix()\n",
    "        self.get_training_labels()\n",
    "        self.get_input_details()\n",
    "        \n",
    "        done = 0\n",
    "        current_classifier = 0\n",
    "        \n",
    "        # this keeps track of what core we are currently generating data for - if all cores have been used or more\n",
    "        # need to be used...\n",
    "        core_count = 0\n",
    "        \n",
    "        no_classifiers = int(0.5 * self.no_classes * (self.no_classes - 1))\n",
    "        print(\"no_classifiers -> \", no_classifiers)\n",
    "        \n",
    "        for n1 in range(self.no_classes):\n",
    "            # loop from zero to (no_classes - 1)\n",
    "            for n2 in range(n1 + 1, self.no_classes):\n",
    "                # loop from (upper loop index + 1) to (no_classes - 1)\n",
    "                                       \n",
    "                # iterate over number of cores\n",
    "                if(core_count < self.no_cores and current_classifier < no_classifiers):\n",
    "                    training_data_1_indices = np.where(self.training_labels_data_fi_uint8 == (n1+1))[0]        # postive class\n",
    "                    training_data_2_indices = np.where(self.training_labels_data_fi_uint8 == (n2+1))[0]        # negative class\n",
    "                    training_data_1_ind_all.append(training_data_1_indices[0])\n",
    "                    training_data_2_ind_all.append(training_data_2_indices[0])\n",
    "                    \n",
    "                    length_class_1 = len(training_data_1_indices)\n",
    "                    length_class_2 = len(training_data_2_indices)\n",
    "                    no_training_vectors = length_class_1 + length_class_2\n",
    "                \n",
    "                    # populate the new training matrix with the two classes in question\n",
    "                    # as training matrix has 2 dimensions, find first and last elements of interest\n",
    "                    first_index_1 = training_data_1_indices[0] * self.no_variables_int\n",
    "                    last_index_1 = (training_data_1_indices[length_class_1 - 1] + 1) * self.no_variables_int\n",
    "                    first_index_2 = training_data_2_indices[0] * self.no_variables_int\n",
    "                    last_index_2 = (training_data_2_indices[length_class_2 - 1] + 1) * self.no_variables_int\n",
    "                    \n",
    "                    training_matrix_new = np.zeros(shape=(no_training_vectors*self.no_variables_int), dtype=np.uint16)\n",
    "                    training_matrix_new[0:length_class_1*self.no_variables_int] = self.training_mat_data_fi_uint16[first_index_1:last_index_1]\n",
    "                    training_matrix_new[length_class_1*self.no_variables_int:(length_class_1*self.no_variables_int+length_class_2*self.no_variables_int)] = self.training_mat_data_fi_uint16[first_index_2:last_index_2]\n",
    "                \n",
    "                    # populate training labels with 1s and -1s\n",
    "                    training_labels_new = np.zeros(shape=(no_training_vectors), dtype=np.int8)\n",
    "                    training_labels_new[0:length_class_1] = 1\n",
    "                    training_labels_new[length_class_1:no_training_vectors] = -1\n",
    "                \n",
    "                    training_matrices.append(training_matrix_new)\n",
    "                    training_labels.append(training_labels_new)\n",
    "                    \n",
    "                    no_training_vectors_all.append(no_training_vectors)\n",
    "                \n",
    "                    current_classifier = current_classifier + 1\n",
    "                    core_count = core_count + 1\n",
    "                    \n",
    "                    self.classifier_indices.append(current_classifier)\n",
    "                    \n",
    "                print(\"current_classifier -> \", current_classifier)\n",
    "                    \n",
    "                if(core_count == self.no_cores or current_classifier == no_classifiers):\n",
    "                    # temporary\n",
    "                    #print(training_matrices)\n",
    "                    #print(training_labels)\n",
    "                    #print(self.classifier_indices)\n",
    "                    \n",
    "                    self.SMO_parallel(training_matrices, training_labels, training_data_1_ind_all, training_data_2_ind_all, no_training_vectors_all, self.no_variables_int, C, tolerance, max_itr)\n",
    "                    core_count = 0\n",
    "                    \n",
    "                    # reset lists to empty for next parallel iteration\n",
    "                    training_matrices = []\n",
    "                    training_labels = []\n",
    "                    no_training_vectors_all = []\n",
    "                    training_data_1_ind_all = []\n",
    "                    training_data_2_ind_all = []\n",
    "                    \n",
    "                    # reset this to empty\n",
    "                    self.classifier_indices = []\n",
    "        \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the training driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMO_driver_inst = SMO_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call driver top-level function\n",
    "All FPGA data processing (e.g. DMA, MMIO, parallel processing...) is abstracted into one function call - there is no need for end-user to understand what the driver does or how the FPGA design has been developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10\n",
    "tolerance = 0.0001\n",
    "max_itr = 10\n",
    "\n",
    "SMO_driver_inst.SMO_driver_top(C, tolerance, max_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_select = 3\n",
    "\n",
    "print(\"offset = \")\n",
    "print(SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.offsets[classifier_select-1]),32,12))\n",
    "print(\"\\n\")\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    print(\"coeff \", loop+1, \" = \", SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.sv_coeffs[classifier_select-1][loop]),32,12))\n",
    "print(\"\\n\")\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    print(\"indice \", loop+1, \" = \", SMO_driver_inst.sv_indices[classifier_select-1][loop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get plane equations (training model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "\n",
    "classifier_select = 1\n",
    "coeffs_1 = []\n",
    "indices_1 = []\n",
    "\n",
    "offset_1 = SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.offsets[classifier_select-1]),32,12)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    coeffs_1.append(SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.sv_coeffs[classifier_select-1][loop]),32,12))\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    indices_1.append(SMO_driver_inst.sv_indices[classifier_select-1][loop])\n",
    "    \n",
    "svs_write = np.reshape(SMO_driver_inst.training_mat_data_fi_uint16,((int(len(SMO_driver_inst.training_mat_data_fi_uint16)/SMO_driver_inst.no_variables_int),SMO_driver_inst.no_variables_int)))\n",
    "svs_write = svs_write[indices_1]\n",
    "\n",
    "svs_1 = np.zeros(shape=(50,3),dtype=np.float32)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])): \n",
    "    for loop_2 in range(SMO_driver_inst.no_variables_int):\n",
    "        svs_1[loop][loop_2] = SMO_driver_inst.fixed_point_to_float(int(svs_write[loop][loop_2]),16,1)\n",
    "    \n",
    "# 2)\n",
    "\n",
    "classifier_select = 2\n",
    "coeffs_2 = []\n",
    "indices_2 = []\n",
    "\n",
    "offset_2 = SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.offsets[classifier_select-1]),32,12)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    coeffs_2.append(SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.sv_coeffs[classifier_select-1][loop]),32,12))\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    indices_2.append(SMO_driver_inst.sv_indices[classifier_select-1][loop])\n",
    "\n",
    "svs_write = np.reshape(SMO_driver_inst.training_mat_data_fi_uint16,((int(len(SMO_driver_inst.training_mat_data_fi_uint16)/SMO_driver_inst.no_variables_int),SMO_driver_inst.no_variables_int)))\n",
    "svs_write = svs_write[indices_2]\n",
    "\n",
    "svs_2 = np.zeros(shape=(50,3),dtype=np.float32)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])): \n",
    "    for loop_2 in range(SMO_driver_inst.no_variables_int):\n",
    "        svs_2[loop][loop_2] = SMO_driver_inst.fixed_point_to_float(int(svs_write[loop][loop_2]),16,1)\n",
    "\n",
    "# 3)\n",
    "\n",
    "classifier_select = 3\n",
    "coeffs_3 = []\n",
    "indices_3 = []\n",
    "\n",
    "offset_3 = SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.offsets[classifier_select-1]),32,12)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    coeffs_3.append(SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.sv_coeffs[classifier_select-1][loop]),32,12))\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    indices_3.append(SMO_driver_inst.sv_indices[classifier_select-1][loop])\n",
    "    \n",
    "svs_write = np.reshape(SMO_driver_inst.training_mat_data_fi_uint16,((int(len(SMO_driver_inst.training_mat_data_fi_uint16)/SMO_driver_inst.no_variables_int),SMO_driver_inst.no_variables_int)))\n",
    "svs_write = svs_write[indices_3]\n",
    "\n",
    "svs_3 = np.zeros(shape=(50,3),dtype=np.float32)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])): \n",
    "    for loop_2 in range(SMO_driver_inst.no_variables_int):\n",
    "        svs_3[loop][loop_2] = SMO_driver_inst.fixed_point_to_float(int(svs_write[loop][loop_2]),16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_select = 1\n",
    "weights_1 = [0,0,0]\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    weights_1 = weights_1 + coeffs_1[loop] * svs_1[loop]\n",
    "    \n",
    "classifier_select = 2\n",
    "weights_2 = [0,0,0]\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    weights_2 = weights_2 + coeffs_2[loop] * svs_2[loop]\n",
    "    \n",
    "classifier_select = 3\n",
    "weights_3 = [0,0,0]\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    weights_3 = weights_3 + coeffs_3[loop] * svs_3[loop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-1, 1, 0.5)\n",
    "y = np.arange(-1, 1, 0.5)\n",
    "\n",
    "#XX, YY = np.meshgrid(range(-1), range(1))\n",
    "XX, YY = np.meshgrid(x,y)\n",
    "\n",
    "# calculate corresponding z\n",
    "#Z = (-weights_1[0] * XX - weights_1[1] * YY - offset_1) * 1. /weights_1[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot planes on training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "#ax = plt.axes(projection='3d')\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "#training_plot_data_x_1 = [float(i) for i in training_plot_data_new[0][0:50]]\n",
    "#training_plot_data_y_1 = [float(i) for i in training_plot_data_new[0][50:100]]\n",
    "#training_plot_data_z_1 = [float(i) for i in training_plot_data_new[0][100:150]]\n",
    "\n",
    "#training_plot_data_x_2 = [float(i) for i in training_plot_data_new[1][0:50]]\n",
    "#training_plot_data_y_2 = [float(i) for i in training_plot_data_new[1][50:100]]\n",
    "#training_plot_data_z_2 = [float(i) for i in training_plot_data_new[1][100:150]]\n",
    "\n",
    "#training_plot_data_x_3 = [float(i) for i in training_plot_data_new[2][0:50]]\n",
    "#training_plot_data_y_3 = [float(i) for i in training_plot_data_new[2][50:100]]\n",
    "#training_plot_data_z_3 = [float(i) for i in training_plot_data_new[2][100:150]]\n",
    "\n",
    "#training_plot_data_x_1 = [float(i) for i in training_plot_data_new[0][0:10]]\n",
    "#training_plot_data_y_1 = [float(i) for i in training_plot_data_new[0][50:60]]\n",
    "#training_plot_data_z_1 = [float(i) for i in training_plot_data_new[0][100:110]]\n",
    "\n",
    "#training_plot_data_x_2 = [float(i) for i in training_plot_data_new[1][0:10]]\n",
    "#training_plot_data_y_2 = [float(i) for i in training_plot_data_new[1][50:60]]\n",
    "#training_plot_data_z_2 = [float(i) for i in training_plot_data_new[1][100:110]]\n",
    "\n",
    "#training_plot_data_x_3 = [float(i) for i in training_plot_data_new[2][0:10]]\n",
    "#training_plot_data_y_3 = [float(i) for i in training_plot_data_new[2][50:60]]\n",
    "#training_plot_data_z_3 = [float(i) for i in training_plot_data_new[2][100:110]]\n",
    "\n",
    "# temp\n",
    "#weights_1 = [-1.5629,0.2301,-0.4966]\n",
    "#offset_1 = -0.0654\n",
    "\n",
    "ax.scatter(training_plot_data_x_1, training_plot_data_y_1, training_plot_data_z_1, c='red', s=1, alpha=1)\n",
    "ax.scatter(training_plot_data_x_2, training_plot_data_y_2, training_plot_data_z_2, c='green', s=1, alpha=1)\n",
    "ax.scatter(training_plot_data_x_3, training_plot_data_y_3, training_plot_data_z_3, c='blue', s=1, alpha=1)\n",
    "\n",
    "Z = -(weights_1[0] * XX + weights_1[1] * YY + offset_1) / weights_1[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='yellow')\n",
    "Z = (-weights_2[0] * XX - weights_2[1] * YY - offset_2) * 1./ weights_2[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='magenta')\n",
    "Z = (-weights_3[0] * XX - weights_3[1] * YY - offset_3) * 1./ weights_3[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='cyan')\n",
    "\n",
    "ax.set_xlabel('1st principle component')\n",
    "ax.set_ylabel('2nd principle component')\n",
    "ax.set_zlabel('3rd principle component')\n",
    "\n",
    "#plt.axis([-1, 1, -1, 1])\n",
    "ax.set_xlim((-1,1))\n",
    "ax.set_ylim((-1,1))\n",
    "ax.set_zlim((-1,1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mat_data = get_testing_matrix()\n",
    "testing_plot_data = np.transpose(np.reshape(testing_mat_data,(150,3)))\n",
    "testing_plot_data_new = testing_plot_data.tolist()\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax = plt.axes(projection='3d')\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "testing_plot_data_x_1 = [float(i) for i in testing_plot_data_new[0][0:50]]\n",
    "testing_plot_data_x_2 = [float(i) for i in testing_plot_data_new[0][50:100]]\n",
    "testing_plot_data_x_3 = [float(i) for i in testing_plot_data_new[0][100:150]]\n",
    "\n",
    "testing_plot_data_y_1 = [float(i) for i in testing_plot_data_new[1][0:50]]\n",
    "testing_plot_data_y_2 = [float(i) for i in testing_plot_data_new[1][50:100]]\n",
    "testing_plot_data_y_3 = [float(i) for i in testing_plot_data_new[1][100:150]]\n",
    "\n",
    "testing_plot_data_z_1 = [float(i) for i in testing_plot_data_new[2][0:50]]\n",
    "testing_plot_data_z_2 = [float(i) for i in testing_plot_data_new[2][50:100]]\n",
    "testing_plot_data_z_3 = [float(i) for i in testing_plot_data_new[2][100:150]]\n",
    "\n",
    "ax.scatter(testing_plot_data_x_1, testing_plot_data_y_1, testing_plot_data_z_1, c='red', s=1, alpha=1)\n",
    "ax.scatter(testing_plot_data_x_2, testing_plot_data_y_2, testing_plot_data_z_2, c='green', s=1, alpha=1)\n",
    "ax.scatter(testing_plot_data_x_3, testing_plot_data_y_3, testing_plot_data_z_3, c='blue', s=1, alpha=1)\n",
    "\n",
    "ax.set_xlabel('1st principle component')\n",
    "ax.set_ylabel('2nd principle component')\n",
    "ax.set_zlabel('3rd principle component')\n",
    "\n",
    "ax.set_xlim((-1,1))\n",
    "ax.set_ylim((-1,1))\n",
    "ax.set_zlim((-1,1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = -(weights_1[0] * XX + weights_1[1] * YY + offset_1) / weights_1[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='yellow')\n",
    "Z = (-weights_2[0] * XX - weights_2[1] * YY - offset_2) * 1./ weights_2[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='magenta')\n",
    "Z = (-weights_3[0] * XX - weights_3[1] * YY - offset_3) * 1./ weights_3[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='cyan')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write training models to files\n",
    "- these will be read by \"deployment\" section which will use them to classify unknown data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file for deployment\n",
    "for n1 in range(SMO_driver_inst.no_classifiers):\n",
    "    SMO_driver_inst.write_training_model_to_files(n1+1)\n",
    "SMO_driver_inst.write_n_svs_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load deployment bitstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETUP load the overlay\n",
    "from pynq import Overlay\n",
    "\n",
    "overlay = Overlay(\"/home/xilinx/jupyter_notebooks/PROJECT_FULL_DEMO/deployment_linear_PYNQ_Z2.bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load deployment drivers - similar to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://pynq.readthedocs.io/en/v2.5/overlay_design_methodology/overlay_tutorial.html\n",
    "# ref: http://www.fpgadeveloper.com/2018/03/how-to-accelerate-a-python-function-with-pynq.html\n",
    "\n",
    "print(\"loading...\")\n",
    "\n",
    "from pynq import DefaultIP\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# the PARSE_FILES class is instantiated once and the all files required for computing the geometric values and test predictions\n",
    "# may be loaded, stored and (saved - if required)\n",
    "class parse_files():\n",
    "    def __init__(self):\n",
    "        #super().__init__()\n",
    "        self.no_variables = None\n",
    "        self.no_variables_int = None\n",
    "        self.no_test_vectors = None\n",
    "        self.no_test_vectors_int= None\n",
    "        self.no_classes_int = None\n",
    "        \n",
    "        # other variables and arrays containing details on the training model and testing set\n",
    "        self.n_svs_data_int = None\n",
    "        self.testing_mat_fi_data_uint16 = None\n",
    "        self.testing_labels_data_int = None\n",
    "        \n",
    "        # these are for each classifier and will need updated several times (for each training model)\n",
    "        self.svs_fi_data_uint16 = None\n",
    "        self.coeff_fi_data_uint32 = None\n",
    "        self.offset_fi_data_uint32 = None\n",
    "        \n",
    "        \n",
    "    def get_ds_details(self):\n",
    "        # read in the dataset details\n",
    "        f = open(\"ds_details.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        ds_details_data = contents.split()\n",
    "        x = np.array(ds_details_data)\n",
    "        ds_details_data_uint32 = np.asarray(x,np.uint32)\n",
    "        #print(type(ds_details_data_uint32[0]))\n",
    "\n",
    "        # no_variables\n",
    "        self.no_variables = ds_details_data_uint32[0]\n",
    "        self.no_variables_int = self.no_variables\n",
    "        # (single-precision floating point 32 bit representation as an integer)\n",
    "        \n",
    "        # no_test_vectors\n",
    "        self.no_test_vectors = ds_details_data_uint32[1]\n",
    "        self.no_test_vectors_int = self.no_test_vectors\n",
    "\n",
    "        # number of classes\n",
    "        self.no_classes_int = ds_details_data_uint32[2]\n",
    "    \n",
    "    def get_no_svs(self):\n",
    "        # read file containing the number of support vectors for each classifier\n",
    "        f = open(\"n_svs.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        n_svs_data = contents.split()\n",
    "        x = np.array(n_svs_data)\n",
    "        self.n_svs_data_int = np.asarray(x,np.uint32)\n",
    "\n",
    "        f.close()\n",
    "        \n",
    "    def get_testing_matrix(self):\n",
    "        f = open(\"test_matrix_fi.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        testing_mat_fi_data = contents.split()\n",
    "        x = np.array(testing_mat_fi_data)\n",
    "        #self.testing_mat_data_uint16 = x.astype(uint16)\n",
    "        self.testing_mat_fi_data_uint16 = np.asarray(x, np.uint16)\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "    #def get_kernel_parameters(self):\n",
    "        \n",
    "    def get_testing_labels(self):\n",
    "        # for self checking Python tests\n",
    "        f = open(\"test_labels.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        testing_labels_data = contents.split()\n",
    "        x = np.array(testing_labels_data)\n",
    "        #testing_labels_data_float = np.asfarray(x,np.float32)\n",
    "        #self.testing_labels_data_int = testing_labels_data_float.astype(int)\n",
    "        self.testing_labels_data_int = np.asarray(x, np.uint8)\n",
    "\n",
    "        f.close()\n",
    "        \n",
    "        f = open(\"test_predictions_libsvm.dat\",\"r\")\n",
    "        \n",
    "        contents = f.read()\n",
    "        testing_labels_data = contents.split()\n",
    "        x = np.array(testing_labels_data)\n",
    "        #testing_labels_data_float = np.asfarray(x,np.float32)\n",
    "        #self.testing_labels_data_int = testing_labels_data_float.astype(int)\n",
    "        self.test_predictions_libsvm = np.asarray(x, np.uint8)\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "    def get_support_vectors(self, current_classifier):\n",
    "        # return support vectors for a particular classifier\n",
    "        file_ext = \".dat\"\n",
    "        svs_file_name = \"svs_fi_\"\n",
    "        svs_file_name_new = svs_file_name + str(current_classifier) + file_ext\n",
    "        \n",
    "        f = open(svs_file_name_new,\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        svs_fi_data = contents.split()\n",
    "        x = np.array(svs_fi_data)\n",
    "        self.svs_fi_data_uint16 = np.asarray(x,np.uint16)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "    def get_sv_coeffs(self, current_classifier):\n",
    "        # store the support vector coefficients for a classifier\n",
    "        file_ext = \".dat\"\n",
    "        coeffs_file_name = \"coeffs_fi_\"\n",
    "        coeffs_file_name_new = coeffs_file_name + str(current_classifier) + file_ext\n",
    "\n",
    "        f = open(coeffs_file_name_new,\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        coeffs_fi_data = contents.split()\n",
    "        x = np.array(coeffs_fi_data)\n",
    "        self.coeffs_fi_data_uint32 = np.asarray(x,np.uint32)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "    def get_offset(self, current_classifier):\n",
    "        # store the offset for a classifier\n",
    "        file_ext = \".dat\"        \n",
    "        offset_file_name  = \"offset_fi_\"\n",
    "        offset_file_name_new = offset_file_name + str(current_classifier) + file_ext\n",
    "        \n",
    "        f = open(offset_file_name_new,\"r\")\n",
    "\n",
    "        offset_fi_data = f.read()\n",
    "        self.offset_fi_data_uint32 = np.asarray(offset_fi_data,np.uint32)\n",
    "    \n",
    "        f.close()\n",
    "    \n",
    "# the GEOMETRIC_VALUES_DRIVER class is instantiated once for each \"geometric_values\" IP core\n",
    "# member functions include loading data to IP core AXI-lite slave interfaces for general design\n",
    "# parameters and generating the contigous buffers to transfer through DMA to the AXI stream (AXIS) \n",
    "# ports on the IP\n",
    "# INHERITS FROM PARSE_FILES\n",
    "from pynq import MMIO\n",
    "\n",
    "import pynq.lib.dma\n",
    "\n",
    "from pynq import allocate\n",
    "#from pynq import Xlnk\n",
    "#xlnk = Xlnk()\n",
    "\n",
    "class deployment_driver(parse_files):\n",
    "    def __init__(self):\n",
    "        #super().__init__()\n",
    "             \n",
    "        # current classifier we are calculating the geometric values for\n",
    "        self.current_classifier = None\n",
    "        \n",
    "        self.geometric_values_out = None\n",
    "        \n",
    "        # get parameters from dat files which are general to all classifiers - i.e. not the support vectors, coefficient or offset\n",
    "        self.get_ds_details()\n",
    "        self.get_no_svs()\n",
    "        self.get_testing_matrix()\n",
    "        \n",
    "        # used for parallel processing of geometric values\n",
    "        self.classifier_indices = []\n",
    "        self.no_classifiers = None\n",
    "        \n",
    "        # LISTS\n",
    "        self.dma_data_instances = []#contains the support vectors followed immediately by testing matrix in C standard type contigous memory\n",
    "        self.dma_cf_instances = []\n",
    "        self.dma_ds_instances = []\n",
    "        self.dma_gv_instances = []\n",
    "        \n",
    "        geometric_values_1 = overlay.geometric_values_1\n",
    "        geometric_values_2 = overlay.geometric_values_2\n",
    "        geometric_values_3 = overlay.geometric_values_3\n",
    "        geometric_values_4 = overlay.geometric_values_4\n",
    "        geometric_values_5 = overlay.geometric_values_5\n",
    "        geometric_values_6 = overlay.geometric_values_6\n",
    "\n",
    "        self.g_v_dispatcher = {\n",
    "            1: geometric_values_1,\n",
    "            2: geometric_values_2,\n",
    "            3: geometric_values_3,\n",
    "            4: geometric_values_4,\n",
    "            5: geometric_values_5,\n",
    "            6: geometric_values_6,\n",
    "        }\n",
    "        \n",
    "        self.geometric_values_all = None\n",
    "        self.test_predictions = None\n",
    "        \n",
    "        self.geometric_values_time = 0\n",
    "        self.test_predictions_time = 0\n",
    "        \n",
    "        ## TEST ##\n",
    "        self.tm_buffer = None\n",
    "        \n",
    "        \n",
    "    def dma_init(self, no_classifiers):\n",
    "        # initialise buffers not required to change on each iteration\n",
    "        #self.test_matrix_buffer = allocate(shape=(self.no_test_vectors_int,self.no_variables_int), dtype=np.uint32)\n",
    "        #np.copyto(self.test_matrix_buffer,self.testing_mat_data_float_IEEE754)\n",
    "        \n",
    "        # store all geometric values here\n",
    "        self.geometric_values_all = np.zeros(shape=(self.no_test_vectors_int,int(no_classifiers)), dtype=np.uint32)\n",
    "        self.test_predictions = np.zeros(shape=(self.no_test_vectors_int,), dtype=np.uint8)\n",
    "        \n",
    "        self.tm_buffer = allocate(shape=(self.no_test_vectors_int*self.no_variables_int,), dtype=np.uint16)\n",
    "        np.copyto(self.tm_buffer,self.testing_mat_fi_data_uint16)\n",
    "        \n",
    "    def dma_delete(self):\n",
    "        self.tm_buffer.close()    \n",
    "        \n",
    "    def dma_transfer_parallel(self, no_classifiers):\n",
    "        # instantiate all DMAs - parallel design\n",
    "        for n1 in range(6):\n",
    "            self.dma_data_instances.append(self.g_v_dispatcher[n1+1].dma_data)\n",
    "            self.dma_cf_instances.append(self.g_v_dispatcher[n1+1].dma_cf)\n",
    "            self.dma_ds_instances.append(self.g_v_dispatcher[n1+1].dma_ds)\n",
    "            self.dma_gv_instances.append(self.g_v_dispatcher[n1+1].dma_gv)\n",
    "            \n",
    "        geometric_values_buffer_1 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)\n",
    "        geometric_values_buffer_2 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)\n",
    "        geometric_values_buffer_3 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)\n",
    "        geometric_values_buffer_4 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)    \n",
    "        geometric_values_buffer_5 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)    \n",
    "        geometric_values_buffer_6 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)    \n",
    "    \n",
    "        geo_values_dispatcher = {\n",
    "            1: geometric_values_buffer_1,\n",
    "            2: geometric_values_buffer_2,\n",
    "            3: geometric_values_buffer_3,\n",
    "            4: geometric_values_buffer_4,\n",
    "            5: geometric_values_buffer_5,\n",
    "            6: geometric_values_buffer_6,\n",
    "        }\n",
    "        \n",
    "        # accumulate with time taken to transfer data to DMA in each classifier\n",
    "        dma_transfer_time = 0\n",
    "                \n",
    "        # iterate over only required classifiers\n",
    "        for n1 in range(6):\n",
    "            if(n1 < len(self.classifier_indices)):\n",
    "                #print(\"test\")\n",
    "                current_classifier = self.classifier_indices[n1]\n",
    "                print(\"current_classifier: \", current_classifier)\n",
    "\n",
    "                # get training model for current classifier to compute geometric values for this classifier\n",
    "                # support vectors:\n",
    "                self.get_support_vectors(current_classifier)\n",
    "                # offset:\n",
    "                self.get_offset(self.classifier_indices[n1])\n",
    "                # support vector coefficients:\n",
    "                self.get_sv_coeffs(self.classifier_indices[n1])\n",
    "\n",
    "                # no_svs is obtained at start from one file\n",
    "                # length of support vectors plus length of testing matrix by 256 variables\n",
    "                svs_length = self.n_svs_data_int[self.classifier_indices[n1]-1] * self.no_variables_int\n",
    "                testing_matrix_length = self.no_test_vectors_int * self.no_variables_int\n",
    "                data_stream_length = svs_length + testing_matrix_length\n",
    "                # length of coeffs plus one (for the offset)\n",
    "                coeffs_stream_length = self.n_svs_data_int[self.classifier_indices[n1]-1] + 1\n",
    "        \n",
    "                #self.data_buffer = allocate(shape=(data_stream_length,), dtype=np.uint16)\n",
    "            \n",
    "                svs_buffer = allocate(shape=(svs_length,), dtype=np.uint16)\n",
    "                coeffs_buffer = allocate(shape=(coeffs_stream_length,), dtype=np.uint32)\n",
    "                ds_buffer = allocate(shape=(3,), dtype=np.uint32)\n",
    "    \n",
    "                #np.copyto(self.data_buffer[0:svs_length],self.svs_fi_data_uint16)\n",
    "                #np.copyto(self.data_buffer[svs_length:data_stream_length],self.testing_mat_fi_data_uint16)\n",
    "            \n",
    "                np.copyto(svs_buffer,self.svs_fi_data_uint16)\n",
    "                np.copyto(coeffs_buffer[0:coeffs_stream_length-1], self.coeffs_fi_data_uint32)\n",
    "                coeffs_buffer[coeffs_stream_length-1] = self.offset_fi_data_uint32\n",
    "\n",
    "                ds_buffer[0] = self.n_svs_data_int[self.classifier_indices[n1]-1]\n",
    "                ds_buffer[1] = self.no_variables_int\n",
    "                ds_buffer[2] = self.no_test_vectors_int\n",
    "                \n",
    "                \n",
    "                # TEMP\n",
    "                #offset = 0\n",
    "                #print(\"sv 1\")\n",
    "                #print(\"Control: \" + hex(self.dma_data_instances[n1].read(0x0 + offset)))\n",
    "                #print(\"Status : \" + hex(self.dma_data_instances[n1].read(0x4 + offset)))                \n",
    "                #offset = 0\n",
    "                #print(\"CF 1\")\n",
    "                #print(\"Control: \" + hex(self.dma_cf_instances[n1].read(0x0 + offset)))\n",
    "                #print(\"Status : \" + hex(self.dma_cf_instances[n1].read(0x4 + offset)))                \n",
    "                # TEMP                \n",
    "                \n",
    "                \n",
    "                # transfer to DMA\n",
    "                start_time = time.time()\n",
    "                #self.dma_data_instances[n1].sendchannel.transfer(self.data_buffer)\n",
    "                self.dma_cf_instances[n1].sendchannel.transfer(coeffs_buffer)\n",
    "                self.dma_ds_instances[n1].sendchannel.transfer(ds_buffer)\n",
    "                self.dma_gv_instances[n1].recvchannel.transfer(geo_values_dispatcher[n1+1])\n",
    "                \n",
    "                self.dma_data_instances[n1].sendchannel.transfer(svs_buffer)\n",
    "                self.dma_data_instances[n1].sendchannel.wait()\n",
    "                self.dma_data_instances[n1].sendchannel.transfer(self.tm_buffer)\n",
    "                \n",
    "                dma_transfer_time = dma_transfer_time + time.time() - start_time\n",
    "                #print(dma_transfer_time)\n",
    "                \n",
    "                \n",
    "                # TEMP\n",
    "                #offset = 0\n",
    "                #print(\"sv 2\")\n",
    "                #print(\"Control: \" + hex(self.dma_data_instances[n1].read(0x0 + offset)))\n",
    "                #print(\"Status : \" + hex(self.dma_data_instances[n1].read(0x4 + offset)))                \n",
    "                #offset = 0\n",
    "                #print(\"CF 2\")\n",
    "                #print(\"Control: \" + hex(self.dma_cf_instances[n1].read(0x0 + offset)))\n",
    "                #print(\"Status : \" + hex(self.dma_cf_instances[n1].read(0x4 + offset)))                  \n",
    "                # TEMP\n",
    "\n",
    "                #self.dma_ds_instances[n1].sendchannel.transfer(ds_buffer)\n",
    "                #self.dma_gv_instances[n1].recvchannel.transfer(geo_values_dispatcher[n1+1])\n",
    "                \n",
    "                #del data_buffer\n",
    "                coeffs_buffer.close()\n",
    "                ds_buffer.close()\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        start_time = time.time()\n",
    "                \n",
    "        for n1 in range(6):\n",
    "            if(n1 < len(self.classifier_indices)):\n",
    "                self.dma_data_instances[n1].sendchannel.wait()\n",
    "                self.dma_cf_instances[n1].sendchannel.wait()\n",
    "                self.dma_ds_instances[n1].sendchannel.wait()\n",
    "                self.dma_gv_instances[n1].recvchannel.wait()\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        elapsed_time = time.time() - start_time + dma_transfer_time\n",
    "        self.geometric_values_time = self.geometric_values_time + elapsed_time\n",
    "        #print(\"GEOMETRIC VALUES TIME: \")\n",
    "        #print(elapsed_time)\n",
    "                \n",
    "        for n1 in range(6):\n",
    "            if(n1 < len(self.classifier_indices)):        \n",
    "                self.geometric_values_all[:,self.classifier_indices[n1]-1] = geo_values_dispatcher[n1+1][:,0]\n",
    "                geo_values_dispatcher[n1+1].close()        \n",
    "    \n",
    "    def geometric_values_driver(self):\n",
    "        # get training model for current classifier to compute geometric values for this classifier\n",
    "        #self.get_support_vectors(current_classifier)\n",
    "        #self.get_sv_coeffs(current_classifier)\n",
    "        #self.get_offset(current_classifier)\n",
    "        \n",
    "        #self.dma_transfer(current_classifier)\n",
    "        \n",
    "        # generate require classifier indices in an 8-length array - there are currently 8 instances of geometric values\n",
    "        # e.g. [1,2,3,4,5,6,7,8] then [9,10] if more than 8 classifiers or just [1,2,3,4,5,6]\n",
    "\n",
    "        # get no_classifiers\n",
    "        no_classifiers = self.no_classes_int * (self.no_classes_int - 1) / 2\n",
    "        self.no_classifiers = no_classifiers\n",
    "        \n",
    "        self.dma_init(no_classifiers)\n",
    "\n",
    "        current_classifier = 1\n",
    "        done = 0\n",
    "        \n",
    "        while(done == 0):\n",
    "            # generate indices - reset to length zero\n",
    "            init_classifier = current_classifier\n",
    "            # (init is the first classifier for the next batch of parallel processing)\n",
    "            self.classifier_indices = []\n",
    "            for n1 in range(6):\n",
    "                if(current_classifier < (no_classifiers + 1)):\n",
    "                    self.classifier_indices.append(init_classifier + n1)\n",
    "                    #self.classifier_indices[n1] = current_classifier + n1\n",
    "                    #if(n1 == 0):\n",
    "                    #    self.classifier_indices[0] = current_classifier + n1\n",
    "                    #else:\n",
    "                    #    np.append(self.classifier_indices, current_classifier + n1)\n",
    "                        \n",
    "                    current_classifier = current_classifier + 1\n",
    "            \n",
    "            #print(\"current (next): \", current_classifier)\n",
    "            if((current_classifier-1) == int(no_classifiers)):\n",
    "                done = 1\n",
    "            \n",
    "            # call dma transfer - parallel calculate geometric values\n",
    "            self.dma_transfer_parallel(no_classifiers)\n",
    "        \n",
    "        self.dma_delete()\n",
    "        \n",
    "    def test_predictions_driver(self):     \n",
    "        no_classes = self.no_classes_int\n",
    "        no_test_vectors = self\n",
    "\n",
    "        dma_gv = overlay.test_predictions_1.dma_gv\n",
    "        dma_ds = overlay.test_predictions_1.dma_ds\n",
    "        dma_tp = overlay.test_predictions_1.dma_tp\n",
    "\n",
    "        ge_values_buffer = allocate(shape=(self.no_test_vectors,int(self.no_classifiers)), dtype=np.uint32)\n",
    "        dataset_buffer = allocate(shape=(2,1), dtype=np.uint32)\n",
    "\n",
    "        np.copyto(ge_values_buffer,self.geometric_values_all)\n",
    "\n",
    "        dataset_buffer[0] = self.no_classes_int\n",
    "        dataset_buffer[1] = self.no_test_vectors\n",
    "\n",
    "        test_predictions_out_buffer = allocate(shape=(self.no_test_vectors,1), dtype=np.uint8)\n",
    "\n",
    "        start_time = time.time()\n",
    "            \n",
    "        # transfer to DMA\n",
    "        dma_gv.sendchannel.transfer(ge_values_buffer)\n",
    "        dma_ds.sendchannel.transfer(dataset_buffer)\n",
    "        dma_tp.recvchannel.transfer(test_predictions_out_buffer)\n",
    "\n",
    "        dma_gv.sendchannel.wait()\n",
    "        dma_ds.sendchannel.wait()\n",
    "        dma_tp.recvchannel.wait()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        self.test_predictions_time = elapsed_time\n",
    "        #print(\"TEST PREDICTIONS TIME: \")\n",
    "        #print(elapsed_time)\n",
    "\n",
    "        self.test_predictions = test_predictions_out_buffer\n",
    "\n",
    "        # delete memory on heap to avoid memory leakage\n",
    "        ge_values_buffer.close()\n",
    "        dataset_buffer.close()\n",
    "        test_predictions_out_buffer.close()\n",
    "        \n",
    "    def get_test_predictions(self):\n",
    "        # get geometric values\n",
    "        self.geometric_values_time = 0\n",
    "        self.test_predictions_time = 0\n",
    "        start_time = time.time()  \n",
    "        \n",
    "        self.geometric_values_driver()\n",
    "        \n",
    "        #elapsed_time = time.time() - start_time\n",
    "        #print(\"TIME (geometric values total): \")\n",
    "        #print(elapsed_time)\n",
    "        \n",
    "        # use geometric values to compute test predictions\n",
    "        #start_time = time.time()  \n",
    "        \n",
    "        self.test_predictions_driver()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"\\nTIME TOTAL (WITH FILE READS): \", elapsed_time)\n",
    "        \n",
    "        print(\"TIME TO RECORD (NOT INCLUDING FILE READS): \", self.geometric_values_time + self.test_predictions_time)\n",
    "\n",
    "print(\"\\ndone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate deployment driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_driver_inst = deployment_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call deployment drivver top-level function\n",
    "-  similar to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_driver_inst.get_test_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print resulting test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deployment_driver_inst.test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## elow function checks how accurate classification is and how consistent it is with MATLAB (double-precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the accuracy of the prediction and simlarity to libsvm result\n",
    "deployment_driver_inst.get_testing_labels()\n",
    "\n",
    "# track errors to compute accuracy of precdiction\n",
    "err_count = 0\n",
    "# track differences to libsvm - this indicates issues with the numerical precision of the algorithm\n",
    "disimilarity_count = 0\n",
    "\n",
    "for i in range(deployment_driver_inst.no_test_vectors_int):\n",
    "    if(deployment_driver_inst.test_predictions[i] != deployment_driver_inst.testing_labels_data_int[i]):\n",
    "        err_count = err_count + 1\n",
    "    if(deployment_driver_inst.test_predictions[i] != deployment_driver_inst.test_predictions_libsvm[i]):\n",
    "        disimilarity_count = disimilarity_count + 1\n",
    "        \n",
    "print(\"accuracy = \", (deployment_driver_inst.no_test_vectors_int - err_count) / deployment_driver_inst.no_test_vectors_int * 100, \"%\")\n",
    "#print(\"similarity = \", (deployment_driver_inst.no_test_vectors_int - disimilarity_count) / deployment_driver_inst.no_test_vectors_int * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot resulting test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "#ax = plt.axes(projection='3d')\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "pred_1_indices = np.where(deployment_driver_inst.test_predictions[:,0] == 1)\n",
    "pred_2_indices = np.where(deployment_driver_inst.test_predictions[:,0] == 2)\n",
    "pred_3_indices = np.where(deployment_driver_inst.test_predictions[:,0] == 3)\n",
    "\n",
    "testing_plot_data_new = np.asarray(testing_plot_data_new)\n",
    "\n",
    "testing_plot_data_x_1 = [float(i) for i in testing_plot_data_new[0][pred_1_indices[0]]]\n",
    "testing_plot_data_x_2 = [float(i) for i in testing_plot_data_new[0][pred_2_indices[0]]]\n",
    "testing_plot_data_x_3 = [float(i) for i in testing_plot_data_new[0][pred_3_indices[0]]]\n",
    "#testing_plot_data_x_1 = [float(i) for i in testing_plot_data_new[0][tuple(pred_1_indices[0])]]\n",
    "#testing_plot_data_x_2 = [float(i) for i in testing_plot_data_new[0][pred_2_indices[0]]]\n",
    "#testing_plot_data_x_3 = [float(i) for i in testing_plot_data_new[0][pred_3_indices[0]]]\n",
    "\n",
    "testing_plot_data_y_1 = [float(i) for i in testing_plot_data_new[1][pred_1_indices[0]]]\n",
    "testing_plot_data_y_2 = [float(i) for i in testing_plot_data_new[1][pred_2_indices[0]]]\n",
    "testing_plot_data_y_3 = [float(i) for i in testing_plot_data_new[1][pred_3_indices[0]]]\n",
    "\n",
    "testing_plot_data_z_1 = [float(i) for i in testing_plot_data_new[2][pred_1_indices[0]]]\n",
    "testing_plot_data_z_2 = [float(i) for i in testing_plot_data_new[2][pred_2_indices[0]]]\n",
    "testing_plot_data_z_3 = [float(i) for i in testing_plot_data_new[2][pred_3_indices[0]]]\n",
    "\n",
    "ax.scatter(testing_plot_data_x_1, testing_plot_data_y_1, testing_plot_data_z_1, c='red', s=1, alpha=1)\n",
    "ax.scatter(testing_plot_data_x_2, testing_plot_data_y_2, testing_plot_data_z_2, c='green', s=1, alpha=1)\n",
    "ax.scatter(testing_plot_data_x_3, testing_plot_data_y_3, testing_plot_data_z_3, c='blue', s=1, alpha=1)\n",
    "\n",
    "Z = -(weights_1[0] * XX + weights_1[1] * YY + offset_1) / weights_1[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='yellow')\n",
    "Z = (-weights_2[0] * XX - weights_2[1] * YY - offset_2) * 1./ weights_2[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='magenta')\n",
    "Z = (-weights_3[0] * XX - weights_3[1] * YY - offset_3) * 1./ weights_3[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='cyan')\n",
    "\n",
    "ax.set_xlabel('1st principle component')\n",
    "ax.set_ylabel('2nd principle component')\n",
    "ax.set_zlabel('3rd principle component')\n",
    "\n",
    "ax.set_xlim((-1,1))\n",
    "ax.set_ylim((-1,1))\n",
    "ax.set_zlim((-1,1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
